# -*- coding: utf-8 -*-
"""Copy of  IR_Group18_rohan.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12KAVh3wrqdGoS9RD7lZHN_7I189lsrqz

***Group Number 18***

Group Members

Manav Jain - 2018A7PS0102G

Rohan Agarwal - 2018A7PS0123G

Mane Digvijaysinh Sujaysinh - 2018A7PS0093G

Vishisht Agarwal - 2018A7PS0224G
"""

import numpy as np

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

document=open("/content/drive/My Drive/IR_assign/documents.txt", 'r')
relevance_assessment=open("/content/drive/My Drive/IR_assign/relevance_assessment.txt", 'r')

"""## **TASK 1**"""



"""PARSING DOCUMENTS FROM QUERIES

"""

docs=document.read()
relevant = relevance_assessment.read()

p=docs.split('/')
p = [x.replace("\n"," ") for x in p]
p=[x.strip() for x in p]
p.pop()

type(relevant)

r=relevant.split('/')
r = [x.replace("\n"," ") for x in r]
r=[x.strip() for x in r]
r.pop()

relevant_dict = {}
relevant_docs = []
for s in r:
  t1 = s.split() 
  t2 = t1[1:]
  relevant_docs.append(t2) 
  for i in t2:
    relevant_dict[i]=1

relevant_dict

relevant_docs

documents={}
for s in p:
  t1 = s.split() 
  t2 = t1[1:] 
  s2 = ' '.join(t2)
  if (t1[0] in relevant_dict.keys()):
      documents[int(t1[0])] = s2

documents

"""INVERTED INDEX

"""

class getInvertedIndex(object):
  def __init__(self,docs):
    self.docs = docs
    self.termList = []
    self.docLists = []
    self.final_dict = {}

    for index in docs.keys():
      for term in docs[index].split(" "):
        if term in self.termList:
          i=self.termList.index(term)
          if (index) not in self.docLists[i]:
            self.docLists[i].append(index)
        else:
          self.termList.append(term)
          self.docLists.append([index])
    inverted_dict={}
    for terms in range(len(self.termList)):
      inverted_dict[self.termList[terms]] = self.docLists[terms]
    dict_len= {key: len(value) for key, value in inverted_dict.items()}
    import operator
    sorted_key_list = sorted(dict_len.items(), key=operator.itemgetter(1), reverse=True)
    sorted_dict = [{item[0]: inverted_dict[item [0]]} for item in sorted_key_list]
    for i in range(len(sorted_dict)):
      for j in sorted_dict[i]:
        self.final_dict[j] = sorted_dict[i][j]
     
  def search(self,term):
    try:
      i=self.termList.index(term)
      return self.docLists[i]
    except:
      return "No results"

myInvertedIndex=getInvertedIndex(documents)

myInvertedIndex.final_dict

"""VOCAB SIZE"""

len (myInvertedIndex.termList)

"""PARSING QUERY"""

queryDoc=open("/content/drive/My Drive/IR_assign/query.txt", 'r')

queryInit=queryDoc.read()

queryFin=queryInit.split('/')
queryFin = [x.replace("\n"," ") for x in queryFin]
queryFin=[x.strip() for x in queryFin]

originalQueries={}
for i in range(len(queryFin)-1):
  s = queryFin[i]
  t1 = s.split()
  t2 = t1[1:] 
  s2 = ' '.join(t2)
  s2 = s2.lower();
  t2 = s2.split();
  originalQueries[int(t1[0])]=[]
  for idx in range(len(t2)):
    originalQueries[int(t1[0])].append(t2[idx])

type(originalQueries)

queries={}
for i in range(1,11):
  queries[i]=[]
  for word in originalQueries[i]:
      queries[i].append(word)

queries

"""PROCESSING 'OR' QUERY

"""

def orQuery(list1, list2):
  ans=[]
  p1=0
  p2=0
  while (p1<len(list1) and p2<len(list2)):
    if (list1[p1] == list2[p2]):
      ans.append(list1[p1])
      p1=p1+1
      p2=p2+1
    else:
      if list1[p1] > list2[p2]:
        ans.append(list2[p2])
        p2=p2+1
      else:
        ans.append(list1[p1])
        p1=p1+1
  while p1<len(list1):
    ans.append(list1[p1])
    p1=p1+1
  while p2<len(list2):
    ans.append(list2[p2])
    p2=p2+1
  return ans

"""RETRIEVING DOCUMENTS WITHOUT ANY LINGUISTIC MODEL"""

docsRetrieved={}
for q in range(1,11):
  docsRetrieved[q]=[]
  for word in queries[q]:
    if word in myInvertedIndex.final_dict:
      docsRetrieved[q] = orQuery(docsRetrieved[q], myInvertedIndex.final_dict[word])

def andQuery(list1, list2):
  ans=0
  p1=0
  p2=0
  while (p1<len(list1) and p2<len(list2)):
    if (list1[p1] == int(list2[p2])):
      ans=ans+1
      p1=p1+1
      p2=p2+1
    else:
      if list1[p1] > int(list2[p2]):
        p2=p2+1
      else:
        p1=p1+1
  return ans

relevant_docs

"""PRECISION & RECALL"""

precision=[]
recall=[]
for q in range(1,11):
  relevantRetrieved=andQuery(docsRetrieved[q], relevant_docs[q-1])
  precision.append(relevantRetrieved/len(docsRetrieved[q]))
  recall.append(relevantRetrieved/len(relevant_docs[q-1]))

import matplotlib.pyplot as plt 
queryID = [i+1 for i in range(len(precision))]
plt.plot(queryID,precision,label = "precision")
plt.plot(queryID,recall,label = "recall")
plt.xlabel('QueryID') 
plt.ylabel('Performance scores') 
plt.title('Performance Graph')
plt.legend() 
plt.show()

"""## **TASK 2**

LINGUISTIC MODELS
"""

import nltk

nltk.download()

import spacy

nlp = spacy.load('en_core_web_sm')

spacy_processed_docs={}
for d in documents.keys():
  spacy_processed_docs[d] = (nlp(documents[d]))

from spacy.lang.en.stop_words import STOP_WORDS
print(STOP_WORDS)
print(type(STOP_WORDS))

"""USING SPACY STOP WORDS TO IMPROVE PRECISION"""

post_stop_word_removal={}
for i in spacy_processed_docs.keys():
  post_stop_word_removal[i] = [token.text for token in spacy_processed_docs[i] if not token.is_stop]

spacyDocs={}
seperator=' '
for i in spacy_processed_docs.keys():
  spacyDocs[i] = seperator.join(post_stop_word_removal[i])

myInvertedIndexP=getInvertedIndex(spacyDocs)

spacy_processed_query=[]
for i in range(1,11):
  temp=seperator.join(originalQueries[i])
  spacy_processed_query.append(nlp(temp))

originalQueries

spacy_processed_query

post_removal={}
for i in range (10):
  post_removal[i] = [token.text for token in spacy_processed_query[i] if not token.is_stop]

post_removal

"""RETRIEVING DOCS AFTER REMOVING SPACY STOP WORDS"""

spacy_docsRetrieved={}
for q in range(10):
  spacy_docsRetrieved[q]=[]
  for word in post_removal[q]:
    if word in myInvertedIndexP.final_dict:
      spacy_docsRetrieved[q] = orQuery(spacy_docsRetrieved[q], myInvertedIndexP.final_dict[word])

"""PRECISION AND RECALL"""

spacy_precision=[]
spacy_recall=[]
for q in range(10):
  relevantRetrieved=andQuery(spacy_docsRetrieved[q], relevant_docs[q])
  spacy_precision.append(relevantRetrieved/len(spacy_docsRetrieved[q]))
  spacy_recall.append(relevantRetrieved/len(relevant_docs[q]))

spacy_queryID = [i+1 for i in range(len(spacy_precision))]
plt.plot(spacy_queryID,spacy_precision,label = "precision")
plt.plot(spacy_queryID,spacy_recall,label = "recall")
plt.xlabel('QueryID') 
plt.ylabel('Performance scores') 
plt.title('Performance Graph')
plt.legend() 
plt.show()

"""USING LANCASTER STEMMER TO IMPROVE RECALL"""

from nltk.stem.snowball import PorterStemmer,SnowballStemmer
from nltk.stem.lancaster import LancasterStemmer

ls = LancasterStemmer()

stemmer_processed_docs={}
for d in documents.keys():
  li = list(documents[d].split(" ")) 
  for i in range(len(li)):
    li[i]=ls.stem(li[i])
  stemmer_processed_docs[d] = seperator.join(li)

myInvertedIndexR=getInvertedIndex(stemmer_processed_docs)

stemmer_processed_query=[]
for i in range(1,11):
  li=[]
  for j in range(len(originalQueries[i])):
    li.append(ls.stem(originalQueries[i][j]))
  stemmer_processed_query.append(seperator.join(li))

"""RETRIEVING DOCS AFTER USING LANCASTER STEMMER"""

stemmer_docsRetrieved={}
for q in range(10):
  stemmer_docsRetrieved[q]=[]
  for word in (stemmer_processed_query[q].split(" ")):
    if word in myInvertedIndexR.final_dict:
      stemmer_docsRetrieved[q] = orQuery(stemmer_docsRetrieved[q], myInvertedIndexR.final_dict[word])

"""PRECISION AND RECALL"""

stemmer_precision=[]
stemmer_recall=[]
for q in range(10):
  relevantRetrieved=andQuery(stemmer_docsRetrieved[q], relevant_docs[q])
  stemmer_precision.append(relevantRetrieved/len(stemmer_docsRetrieved[q]))
  stemmer_recall.append(relevantRetrieved/len(relevant_docs[q]))

stemmer_queryID = [i+1 for i in range(len(stemmer_precision))]
plt.plot(stemmer_queryID,stemmer_precision,label = "precision")
plt.plot(stemmer_queryID,stemmer_recall,label = "recall")
plt.xlabel('QueryID') 
plt.ylabel('Performance scores') 
plt.title('Performance Graph')
plt.legend() 
plt.show()

"""##**TASK 3**

***Task3 A:***
Changes made in the dataset for improving on precision:
Removal of stop-words using spacy:
We observed that there were instances of documents having a lot of stop words like “as”, “in”  etc. Owing to these, a lot of non-relevant documents were getting retrieved.
	
Changes made in the dataset for improving on recall:
We did try to remove stop words, using spacy, but, unexpectedly this led to a decrease in recall. 
*For Ex*. One document had a lot of stop words like “is”, “an” etc. but also had a relevant word *“mechanisms”*. Even though my query had *“mechanism”* but the reason it was initially retrieved was because of overlapping stop words. Now that all the stop words were removed and no other linguistic model was used, this document did not get retrieved.
Use of stemming:
We decided to go with Porter’s stemmer but we observed that only the following conversions were being made equivalent: 
Controlled --- control
We tried LancasterStemmer after that and in addition to the making the above two words equivalent, it was also making the following conversions:
Amplifiers --- amplification

***Task3 B:***
Changes in Recall for respective queries:
Before using stemming :
[1.00, 1.00, 1.00, 1.00, 1.00, 0.96, 0.91, 0.87, 1.00, 1.00]
After using stemming:
[1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.95, 0.87, 1.00, 1.00]                              

 Changes in Precision for respective queries:
 Before stopword removal:
[0.15, 0.17, 0.07, 0.02, 0.13, 0.30, 0.13, 0.11, 0.24, 0.04]
After stopword removal:
[0.36, 0.36, 0.50, 0.12, 0.37, 0.30, 0.35, 0.12, 0.24, 0.22]

## **TASK 4**
"""

def and_query(list1, list2):
  ans=[]
  p1=0
  p2=0
  while (p1<len(list1) and p2<len(list2)):
    if (list1[p1] == list2[p2]):
      ans.append(list1[p1])
      p1=p1+1
      p2=p2+1
    else:
      if list1[p1] > list2[p2]:
        p2=p2+1
      else:
        p1=p1+1
  return ans

bi_gram_dict={}
for word in myInvertedIndex.final_dict.keys():
  temp = '$' + word + '$'
  for i in range(len(temp)-1):
    bi = temp[i]+temp[i+1]
    if (bi in bi_gram_dict.keys()):
      if (word not in bi_gram_dict[bi]):
        bi_gram_dict[bi].append(word)
    else:
      bi_gram_dict[bi] = [word]

for bi in bi_gram_dict.keys():
  bi_gram_dict[bi].sort()

for q in queries.keys():
  queries[q][2] = queries[q][2][:2] + '*' + queries[q][2][2:]
  queries[q][4] = '*' + queries[q][4]
  queries[q][1] = queries[q][1] + '*'

doc_ret = {}
for q in queries.keys():
  doc_ret[q] = []
  for word in queries[q]:
    if ('*' in word):
      temp = '$'+word+'$'
      
      word_list = []
      f=0
      for i in range(len(temp)-1):
        if (temp[i]!='*' and temp[i+1]!='*'):
          bi = temp[i]+temp[i+1]
          if (f==0):
            word_list = bi_gram_dict[bi]
            f=1
          else:
            word_list = and_query(word_list, bi_gram_dict[bi])
      for w in word_list:
        if (w in myInvertedIndex.final_dict.keys()):
          doc_ret[q] = orQuery(doc_ret[q], myInvertedIndex.final_dict[w])

    else:
      if (word in myInvertedIndex.final_dict.keys()):
        doc_ret[q] = orQuery(doc_ret[q], myInvertedIndex.final_dict[word])

"""PRECISION AND RECALL WITHOUT ANY LINGUISTIC MODEL"""

bi_precision=[]
bi_recall=[]
for q in range(1,11):
  relevantRetrieved=andQuery(doc_ret[q], relevant_docs[q-1])
  bi_precision.append(relevantRetrieved/len(doc_ret[q]))
  bi_recall.append(relevantRetrieved/len(relevant_docs[q-1]))

print(bi_precision)
print(bi_recall)

plt.plot(queryID,bi_precision,label = "precision")
plt.plot(queryID,bi_recall,label = "recall")
plt.xlabel('QueryID') 
plt.ylabel('Performance scores') 
plt.title('Performance Graph')
plt.legend() 
plt.show()

bi_gram_dict_P={}
for word in myInvertedIndexP.final_dict.keys():
  temp = '$' + word + '$'
  for i in range(len(temp)-1):
    bi = temp[i]+temp[i+1]
    if (bi in bi_gram_dict_P.keys()):
      if (word not in bi_gram_dict_P[bi]):
        bi_gram_dict_P[bi].append(word)
    else:
      bi_gram_dict_P[bi] = [word]

for bi in bi_gram_dict_P.keys():
  bi_gram_dict_P[bi].sort()

for q in post_removal.keys():
  post_removal[q][2] = post_removal[q][2][:2] + '*' + post_removal[q][2][2:]
  post_removal[q][4] = '*' + post_removal[q][4]
  post_removal[q][1] = post_removal[q][1] + '*'

doc_ret_P = {}
for q in post_removal.keys():
  doc_ret_P[q] = []
  for word in post_removal[q]:
    if ('*' in word):
      temp = '$'+word+'$'
      
      word_list = []
      f=0
      for i in range(len(temp)-1):
        if (temp[i]!='*' and temp[i+1]!='*'):
          bi = temp[i]+temp[i+1]
          if (f==0):
            word_list = bi_gram_dict_P[bi]
            f=1
          else:
            word_list = and_query(word_list, bi_gram_dict_P[bi])
  
      for w in word_list:
        if (w in myInvertedIndexP.final_dict.keys()):
          doc_ret_P[q] = orQuery(doc_ret_P[q], myInvertedIndexP.final_dict[w])

    else:
      if (word in myInvertedIndexP.final_dict.keys()):
        doc_ret_P[q] = orQuery(doc_ret_P[q], myInvertedIndexP.final_dict[word])

"""PRECISION AND RECALL USING SPACY STOPWORDS REMOVAL"""

bi_precision_P=[]
bi_recall_P=[]
for q in range(10):
  relevantRetrieved=andQuery(doc_ret_P[q], relevant_docs[q])
  bi_precision_P.append(relevantRetrieved/len(doc_ret_P[q]))
  bi_recall_P.append(relevantRetrieved/len(relevant_docs[q]))

print(bi_precision_P)
print(bi_recall_P)

plt.plot(queryID,bi_precision_P,label = "precision")
plt.plot(queryID,bi_recall_P,label = "recall")
plt.xlabel('QueryID') 
plt.ylabel('Performance scores') 
plt.title('Performance Graph')
plt.legend() 
plt.show()

stemmer_query = {}
for i in range(10):
  stemmer_query[i] = stemmer_processed_query[i].split(' ')

for q in stemmer_query.keys():
  stemmer_query[q][2] = stemmer_query[q][2][:2] + '*' + stemmer_query[q][2][2:]
  stemmer_query[q][4] = '*' + stemmer_query[q][4]
  stemmer_query[q][1] = stemmer_query[q][1] + '*'

bi_gram_dict_R={}
for word in myInvertedIndexR.final_dict.keys():
  temp = '$' + word + '$'
  for i in range(len(temp)-1):
    bi = temp[i]+temp[i+1]
    if (bi in bi_gram_dict_R.keys()):
      if (word not in bi_gram_dict_R[bi]):
        bi_gram_dict_R[bi].append(word)
    else:
      bi_gram_dict_R[bi] = [word]

for bi in bi_gram_dict_R.keys():
  bi_gram_dict_R[bi].sort()

doc_ret_R = {}
for q in stemmer_query.keys():
  doc_ret_R[q] = []
  for word in stemmer_query[q]:
    if ('*' in word):
      temp = '$'+word+'$'
      
      word_list = []
      f=0
      for i in range(len(temp)-1):
        if (temp[i]!='*' and temp[i+1]!='*'):
          bi = temp[i]+temp[i+1]
          if (f==0):
            if (bi in bi_gram_dict_R.keys()):
              word_list = bi_gram_dict_R[bi]
              f=1
          else:
            if (bi in bi_gram_dict_R.keys()):
              word_list = and_query(word_list, bi_gram_dict_R[bi])
      if (q==1):
        print(temp,word_list)
      for w in word_list:
        if (w in myInvertedIndexR.final_dict.keys()):
          doc_ret_R[q] = orQuery(doc_ret_R[q], myInvertedIndexR.final_dict[w])

    else:
      if (word in myInvertedIndexR.final_dict.keys()):
        doc_ret_R[q] = orQuery(doc_ret_R[q], myInvertedIndexR.final_dict[word])

"""PRECISION AND REMOVAL USING LANCASTER STEMMER"""

bi_precision_R=[]
bi_recall_R=[]
for q in range(10):
  relevantRetrieved=andQuery(doc_ret_R[q], relevant_docs[q])
  bi_precision_R.append(relevantRetrieved/len(doc_ret_R[q]))
  bi_recall_R.append(relevantRetrieved/len(relevant_docs[q]))

print(bi_precision_R)
print(bi_recall_R)

plt.plot(queryID,bi_precision_R,label = "precision")
plt.plot(queryID,bi_recall_R,label = "recall")
plt.xlabel('QueryID') 
plt.ylabel('Performance scores') 
plt.title('Performance Graph')
plt.legend() 
plt.show()

"""JUSTIFICATION :- Different wild card queries were made for all the different models and processed with the help of bi-gram indexing. The results of Precision and Recall of different models were consistent with the previous results described above. 

The precision of the full model with wildcard queries was increased with the help of using spacy stop word removal on the wildcard queries, while the recall was improved by employing Lancaster stemmer for wildcard queries.

ASSIGNMENT-2 starts from here!!!!

all variables used in the second assignment must end with 2.
"""

# originalQueries

import nltk
from nltk.stem.porter import *
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords

def removeStopWords(tokenizedSent):
  '''
  :params:
  tokenizedSent: list of individual tokens

  :returns:
  list of individual tokens w/o the stop words
  '''
  sw = set(stopwords.words('english'))
  return [w for w in tokenizedSent if w not in sw]

post_stop_removal2={}
for i in range (1,11):
  post_stop_removal2[i] = removeStopWords(originalQueries[i])

for i in range(1,11):
  for s in post_stop_removal2[i]:
    s=s.lower()

## HELPER FUNCTIONS

def lowerSent(sentence):
  '''
  :params:
  sentence: complete input string

  :returns:
  lower-cased input string
  '''
  return sentence.lower()

def tokenizeSent(sentence):
  '''
  :params:
  sentence: complete input string

  :returns:
  list of individual tokens
  '''
  tokenizer = RegexpTokenizer("[a-zA-Z@]+")
  return tokenizer.tokenize(sentence)

def removeStopWords(tokenizedSent):
  '''
  :params:
  tokenizedSent: list of individual tokens

  :returns:
  list of individual tokens w/o the stop words
  '''
  sw = set(stopwords.words('english'))
  return [w for w in tokenizedSent if w not in sw]
  
def stem(tokenizedSent):
  '''
  :params:
  tokenizedSent: list of individual tokens

  :returns:
  list of stemmed individual tokens
  '''
  stemmer = PorterStemmer()
  return [stemmer.stem(w) for w in tokenizedSent]

def getTokens(sentence):
  tokens=[]
  t1=[]
  if type(sentence) is list:
    t1 = sentence
  else:
    t1 = sentence.split() 
  t2 = t1[1:]
  t3 = removeStopWords(t2)
  return t3

from collections import defaultdict,Counter

def get_tf_doc(docs,doc_id):
    """
    :params:
    docId: Id of a document
    docs : dictionary of documents
    :returns:
    a dictionary correpsonding to token:frequency(token) of tokens present in the document represented by docId 
  """
    tokens=getTokens(docs[doc_id])
    tf_dict = Counter(tokens)
    return tf_dict

doc_ids=[]
for ids in documents.keys():
  doc_ids.append(ids)

tf_dict_full = {}
for docId in doc_ids:
    tf_dict_full[docId] = get_tf_doc(documents,docId)

# tf_dict_full

def get_df(documents):
    '''
    :params:
    documents: dict of documents [corpus]

    :returns:
     a dictionary corresponding to token:Document Frequency(token) i.e the number of documents that token is present in
    '''

    result_dict = None
    for doc in documents.keys():
        doc_tokens = getTokens(documents[doc])
        temp_df_dict = Counter(set(doc_tokens))
        if result_dict:
            result_dict = result_dict + temp_df_dict
        else:
            result_dict = temp_df_dict
    return result_dict

df_dict_full=get_df(documents)

from math import log10

idf_dict_full = {}
for key,val in df_dict_full.items():
    idf_dict_full[key] = log10(len(documents)/float(val)) 
    ## AT TIMES N/(1+ df) IS USED TO HANDLE THE CASE WHERE DF = 0, HOWEVER, THAT CAN LEAD TO IDF BEING NEGATIVE

def TFIDFRetrieval(query, documents, docs_tf_dict=None, idf_dict=None):
  '''
  :params:
  query: user input [query string]
  documents: set of documents [corpus]
  docs_tf_dict: dictionary of term-frequencies for each document [OPTIONAL]
  idf_dict: dictionary of inverse document frequencies for each term in corpus [OPTIONAL]

  :returns:
  list of sorted [decreasing] tuples where each tuple comprises of the document ID and the associated score
  '''
  if docs_tf_dict == None:
    docs_tf_dict = {}
    for docId in documents.keys():
      docs_tf_dict[docId] = get_tf_doc(documents,docId)
  
  if idf_dict == None:
    df_dict = get_df(documents)
    idf_dict = {}
    for key,val in df_dict.items():
      idf_dict[key] = log10(len(spacyDocs)/float(val))

  query_tokens = query
  results = []
  for doc in documents.keys():
    score = 0
    for token in query_tokens:
      try:
        score += (log10(docs_tf_dict[doc][token]+1) * idf_dict[token])
      except:
        score += 0
    results.append([score,doc])
  return sorted(results, reverse=True)

import math

def truncate(number, decimals=0):
    """
    Returns a value truncated to a specific number of decimal places.
    """
    if not isinstance(decimals, int):
        raise TypeError("decimal places must be an integer.")
    elif decimals < 0:
        raise ValueError("decimal places has to be 0 or more.")
    elif decimals == 0:
        return math.trunc(number)

    factor = 10.0 ** decimals
    return math.trunc(number * factor) / factor

from prettytable import PrettyTable
    
x = PrettyTable()

x.field_names = ["Query IDs", "Doc IDs", "Scores"]

listRow=[]
for qid,q in post_stop_removal2.items():
  results=TFIDFRetrieval(q,documents, docs_tf_dict=tf_dict_full, idf_dict=idf_dict_full)
  templist1=[]
  templist2=[]
  listRow.append(qid)
  for i in range (20):
    templist1.append(results[i][1])
    templist2.append(truncate(results[i][0],3))
  listRow.append(templist1)
  listRow.append(templist2)
  x.add_row(listRow)
  listRow.clear()

print(x)

"""Top K precision without any modification"""

rel_docs_dict={}
rel_docs_num={}
for str in r:
  t1=str.split(' ');
  t2=t1[1:]
  q_no=int(t1[0])
  num=0
  for s in t2:
    if s=='':
      continue
    s=int(s)
    num+=1
    if s not in rel_docs_dict:
      rel_docs_dict[s]=[]
    rel_docs_dict[s].append(q_no)
  rel_docs_num[q_no]=num

precision_k_full={}
recall_k_full={}
total=0
rel=0
for qid,q in post_stop_removal2.items():
  total=0
  rel=0
  results=TFIDFRetrieval(q,documents, docs_tf_dict=tf_dict_full, idf_dict=idf_dict_full)
  precision_k_full[qid]=[]
  recall_k_full[qid]=[]
  for i in range (20):
    total+=1
    if results[i][1] in rel_docs_dict:
      for s in rel_docs_dict[results[i][1]]:
        if(s==qid):
          rel+=1
          break
    precision_k_full[qid].append(rel/total)
    recall_k_full[qid].append(rel/rel_docs_num[qid])

"""**Increase** **Precision**"""

# spacyDocs

def myTokenizer(sentence):
  '''
  :params:
  sentence: complete input string

  :returns:
  list of preprocessed individual tokens
  '''
  loweredSent = lowerSent(sentence)
  tokenizedSent = tokenizeSent(loweredSent)
  tokenizedSent = removeStopWords(tokenizedSent)
  stemmedSent = stem(tokenizedSent)
  preprocessedSent = stemmedSent

  return preprocessedSent

def get_tf_doc_p(docs,doc_id):
    """
    :params:
    docId: Id of a document

    :returns:
    a dictionary correpsonding to token:frequency(token) of tokens present in the document represented by docId 
  
"""

    tokens=myTokenizer(docs[doc_id])
    tf_dict = Counter(tokens)
    return tf_dict

tf_dict_p={}
for docId in doc_ids:
    tf_dict_p[docId] = get_tf_doc_p(documents,docId)

def get_df_p(documents):
    '''
    :params:
    doc_ids: set of documents [corpus]

    :returns:
     a dictionary corresponding to token:Document Frequency(token) i.e the number of documents that token is present in
    '''

    result_dict = None
    for doc in documents.keys():
        doc_tokens = myTokenizer(documents[doc])
        temp_df_dict = Counter(set(doc_tokens))
        if result_dict:
            result_dict = result_dict + temp_df_dict
        else:
            result_dict = temp_df_dict
    return result_dict

df_dict_p=get_df_p(documents)

idf_dict_p = {}
for key,val in df_dict_p.items():
    idf_dict_p[key] = log10(len(documents)/float(val)) 
    ## AT TIMES N/(1+ df) IS USED TO HANDLE THE CASE WHERE DF = 0, HOWEVER, THAT CAN LEAD TO IDF BEING NEGATIVE

stemmed_queries2={}
for qid,q in post_stop_removal2.items():
  stemmed_queries2[qid]=stem(q)

x = PrettyTable()

x.field_names = ["Query IDs", "Doc IDs", "Scores"]

listRow=[]
for qid,q in stemmed_queries2.items():
  results=TFIDFRetrieval(q,documents, docs_tf_dict=tf_dict_p, idf_dict=idf_dict_p)
  templist1=[]
  templist2=[]
  listRow.append(qid)
  for i in range (20):
    templist1.append(results[i][1])
    templist2.append(truncate(results[i][0],3))
  listRow.append(templist1)
  listRow.append(templist2)
  x.add_row(listRow)
  listRow.clear()

print(x)

precision_k_p={}
recall_k_p={}
total=0
rel=0
total_rel=0
for qid,q in stemmed_queries2.items():
  total=0
  rel=0
  results=TFIDFRetrieval(q,documents, docs_tf_dict=tf_dict_p, idf_dict=idf_dict_p)
  precision_k_p[qid]=[]
  recall_k_p[qid]=[]
  for i in range (20):
    total+=1
    if results[i][1] in rel_docs_dict:
      for s in rel_docs_dict[results[i][1]]:
        if(s==qid):
          rel+=1
          break
    precision_k_p[qid].append(rel/total)
    recall_k_p[qid].append(rel/rel_docs_num[qid])

prec_2_full = []
prec_2_p = []
for i in precision_k_full.keys():
  prec_2_full.append(precision_k_full[i][1])
  prec_2_p.append(precision_k_p[i][1])
plt.plot(queryID,prec_2_full,label = "Original Precision")
plt.plot(queryID,prec_2_p,label = "Improved Precision")
plt.xlabel('QueryID') 
plt.ylabel('Performance scores') 
plt.title('Performance Graph for K=2')
plt.legend() 
plt.figure(figsize = (20, 18))
plt.show() 

prec_5_full = []
prec_5_p = []
for i in precision_k_full.keys():
  prec_5_full.append(precision_k_full[i][4])
  prec_5_p.append(precision_k_p[i][4])
plt.plot(queryID,prec_5_full,label = "Original Precision")
plt.plot(queryID,prec_5_p,label = "Improved Precision")
plt.xlabel('QueryID') 
plt.ylabel('Performance scores') 
plt.title('Performance Graph for K=5')
plt.legend() 
plt.figure(figsize = (20, 18))
plt.show()


prec_10_full = []
prec_10_p = []
for i in precision_k_full.keys():
  prec_10_full.append(precision_k_full[i][9])
  prec_10_p.append(precision_k_p[i][9])
plt.plot(queryID,prec_10_full,label = "Original Precision")
plt.plot(queryID,prec_10_p,label = "Improved Precision")
plt.xlabel('QueryID') 
plt.ylabel('Performance scores') 
plt.title('Performance Graph for K=10')
plt.legend() 
plt.figure(figsize = (20, 18))
plt.show()

import matplotlib
import matplotlib.pyplot as plt
import numpy as np


# labels = ['G1', 'G2', 'G3', 'G4', 'G5']
labels=[]
prec=[]
rec=[]
#k=2
for qid in stemmed_queries2.keys():
  labels.append(qid)
  prec.append(precision_k_p[qid][1])
  rec.append(recall_k_p[qid][1])


x = np.arange(len(labels))*2  # the label locations
width = 0.6  # the width of the bars

fig, ax = plt.subplots()
rects1 = ax.bar(x - width/2, prec, width, label='precision')
rects2 = ax.bar(x + width/2, rec, width, label='recall')

# Add some text for labels, title and custom x-axis tick labels, etc.
ax.set_ylabel('Scores')
ax.set_title('Top k precision with k=2')
ax.set_xticks(x)
ax.set_xticklabels(labels)
ax.legend()


def autolabel(rects):
    """Attach a text label above each bar in *rects*, displaying its height."""
    for rect in rects:
        height = rect.get_height()
        ax.annotate('{}'.format(truncate(height,2)),
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')


autolabel(rects1)
autolabel(rects2)

fig.tight_layout()
fig.set_size_inches(16, 8)
plt.show()

import matplotlib
import matplotlib.pyplot as plt
import numpy as np


# labels = ['G1', 'G2', 'G3', 'G4', 'G5']
labels=[]
prec=[]
rec=[]
#k=2
for qid in stemmed_queries2.keys():
  labels.append(qid)
  prec.append(precision_k_p[qid][4])
  rec.append(recall_k_p[qid][4])


x = np.arange(len(labels))*2  # the label locations
width = 0.6  # the width of the bars

fig, ax = plt.subplots()
rects1 = ax.bar(x - width/2, prec, width, label='precision')
rects2 = ax.bar(x + width/2, rec, width, label='recall')

# Add some text for labels, title and custom x-axis tick labels, etc.
ax.set_ylabel('Scores')
ax.set_title('Top k precision with k=5')
ax.set_xticks(x)
ax.set_xticklabels(labels)
ax.legend()


def autolabel(rects):
    """Attach a text label above each bar in *rects*, displaying its height."""
    for rect in rects:
        height = rect.get_height()
        ax.annotate('{}'.format(truncate(height,2)),
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')


autolabel(rects1)
autolabel(rects2)

fig.tight_layout()
fig.set_size_inches(16, 8)
plt.show()

import matplotlib
import matplotlib.pyplot as plt
import numpy as np


# labels = ['G1', 'G2', 'G3', 'G4', 'G5']
labels=[]
prec=[]
rec=[]
#k=2
for qid in stemmed_queries2.keys():
  labels.append(qid)
  prec.append(precision_k_p[qid][9])
  rec.append(recall_k_p[qid][9])


x = np.arange(len(labels))  # the label locations
width = 0.6  # the width of the bars
x = x*2
fig, ax = plt.subplots()
rects1 = ax.bar(x - width/2, prec, width, label='precision')
rects2 = ax.bar(x + width/2, rec, width, label='recall')

# Add some text for labels, title and custom x-axis tick labels, etc.
ax.set_ylabel('Scores')
ax.set_title('Top k precision with k=10')
ax.set_xticks(x)
ax.set_xticklabels(labels)
ax.legend()


def autolabel(rects):
    """Attach a text label above each bar in *rects*, displaying its height."""
    for rect in rects:
        height = rect.get_height()
        ax.annotate('{}'.format(truncate(height,2)),
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')


autolabel(rects1)
autolabel(rects2)

fig.tight_layout()
fig.set_size_inches(16, 8)

plt.show()

"""## Task2"""

import copy
spell_error_queries = copy.deepcopy(post_stop_removal2)

import random
import string
for query_id in spell_error_queries.keys():
  temp = []
  for i in range(len(spell_error_queries[query_id] )):
    if len(spell_error_queries[query_id][i])>2:
      temp.append(i)
  r1 = random.choice(temp)
  w1 = spell_error_queries[query_id][r1]
  delchar = random.randint(1,len(w1)-1)
  tw1 = w1[:delchar-1]+ w1[delchar:]
  spell_error_queries[query_id][r1] = tw1
  temp.remove(r1)

  r2 = random.choice(temp)
  w2 = spell_error_queries[query_id][r2]
  delchar = random.randint(1,len(w2)-1)
  tw2 = w2[:delchar-1]+ w2[delchar:]
  r22 = random.randint(0,len(tw2)-1)
  tw22 = tw2[:r22] + random.choice(string.ascii_lowercase) + tw2[r22:]
  spell_error_queries[query_id][r2] = tw22
  temp.remove(r2)

  r3 = random.choice(temp)
  w3 = spell_error_queries[query_id][r3]
  l = len(w3)
  pos1 = random.randint(0,l//2-1)
  pos2 = random.randint(l//2+1,l-1)
  w33 = [char for char in  w3]
  c = w33[pos1]
  w33[pos1]= w33[pos2]
  w33[pos2] = c
  fw3 = ''.join(w33)
  spell_error_queries[query_id][r3] = fw3
  #print(w1,w2,w3)

def editDistDP(str1, str2):
    m = len(str1);
    n = len(str2); 
    dp = [[0 for x in range(n + 1)] for x in range(m + 1)] 
    for i in range(m + 1): 
        for j in range(n + 1): 
            if i == 0: 
                dp[i][j] = j 
            elif j == 0: 
                dp[i][j] = i 
            elif str1[i-1] == str2[j-1]: 
                dp[i][j] = dp[i-1][j-1] 
            else: 
                dp[i][j] = min(1+dp[i][j-1], 1+dp[i-1][j], 2+dp[i-1][j-1])
  
    return dp[m][n]

myInvertedIndex2 = getInvertedIndex(documents)

corrected_queries = {}
from prettytable import PrettyTable
t = PrettyTable(['Query IDs', 'Typo', 'Term', 'Distance'])
for i in spell_error_queries.keys():
  corrected_queries[i] = []
  for word in spell_error_queries[i]:
    mini = 5
    corr_word = word
    listRow = [];
    listRow.append(i);
    listRow.append(word);
    for original_word in myInvertedIndex2.final_dict.keys():
      if original_word in STOP_WORDS:
        continue
      if (editDistDP(word, original_word)<mini):
        mini = editDistDP(word, original_word)
        corr_word = original_word
    listRow.append(corr_word);
    listRow.append(mini);
    corrected_queries[i].append(corr_word)
    if corr_word != word:
      t.add_row(listRow);
print(t);

precision_k_spell_with_error={}
total=0
rel=0
for qid,q in spell_error_queries.items():
  total=0
  rel=0
  results=TFIDFRetrieval(q,documents)
  precision_k_spell_with_error[qid]=[]
  for i in range(20):
    total+=1
    if results[i][1] in rel_docs_dict:
      for s in rel_docs_dict[results[i][1]]:
        if(s==qid):
          rel+=1
          break
    precision_k_spell_with_error[qid].append(rel/total)
    print("{0:d} ".format(results[i][1]),end="\t")
  for i in range (20):
    print("{0:2.2f} " .format(results[i][0]),end="\t")
  print()
precision_k_spell_with_error

precision_k_spell={}
total=0
rel=0
for qid,q in corrected_queries.items():
  total=0
  rel=0
  results=TFIDFRetrieval(q,documents)
  precision_k_spell[qid]=[]
  for i in range(20):
    total+=1
    if results[i][1] in rel_docs_dict:
      for s in rel_docs_dict[results[i][1]]:
        if(s==qid):
          rel+=1
          break
    precision_k_spell[qid].append(rel/total)
    print("{0:d} ".format(results[i][1]),end="\t")
  for i in range (20):
    print("{0:2.2f} " .format(results[i][0]),end="\t")
  print()
precision_k_spell

prec_5_error = []
prec_5_corrected = []
for i in precision_k_spell_with_error.keys():
  prec_5_error.append(precision_k_spell_with_error[i][4])
  prec_5_corrected.append(precision_k_spell[i][4])
plt.plot(queryID,prec_5_error,label = "Original Precision")
plt.plot(queryID,prec_5_corrected,label = "Improved Precision")
plt.xlabel('QueryID') 
plt.ylabel('Performance scores') 
plt.title('Performance Graph for K=5')
plt.legend() 
plt.figure(figsize = (20, 18))
plt.show()


prec_10_error = []
prec_10_corrected = []
for i in precision_k_spell_with_error.keys():
  prec_10_error.append(precision_k_spell_with_error[i][9])
  prec_10_corrected.append(precision_k_spell[i][9])
plt.plot(queryID,prec_10_error,label = "Original Precision")
plt.plot(queryID,prec_10_corrected,label = "Improved Precision")
plt.xlabel('QueryID') 
plt.ylabel('Performance scores') 
plt.title('Performance Graph for K=5')
plt.legend() 
plt.figure(figsize = (20, 18))
plt.show()

#fingerkeying - an implementation of QWERTY Keyboard
#Author Ekta Grover ekta1007@gmail.com on 19th July, 2013

from __future__ import division
from itertools import chain
#import pydot
from random import random
import matplotlib.pyplot as plt
import networkx as nx


G=nx.Graph()
# Keying in the structure of the QWERTY keyboard
row1=['q','w','e','r','t','y','u','i','o','p']
row2=['a','s','d','f','g','h','j','k','l']
row3=['z','x','c','v','b','n','m']

# Dividing the keyboard in left and right 

right_keys=list(chain.from_iterable([row1[0:5],row2[0:5],row3[0:5]]))
left_keys=list(chain.from_iterable([row1[5:10],row2[5:9],row3[5:7]]))

# add each rows independetly with the corresponding weights - we will use only weight of 1 , so we connect only adjacent nodes
for i in range(0,len(row1)-1):
    G.add_edge(row1[i],row1[i+1],weight=1,color='red')
  
for i in range(0,len(row2)-1):
    G.add_edge(row2[i],row2[i+1],weight=1,color='blue')
    
for i in range(0,len(row3)-1):
    G.add_edge(row3[i],row3[i+1],weight=1,color='green')

# add the next row in there - row1 with row2
for i in range(0, len(row1)):
    if i==0:
        G.add_edge(row1[i],row2[i],weight=1,color='cyan')
        G.add_edge(row1[i],row2[i+1],weight=1,color='cyan')
    elif i<= 7 :
        G.add_edge(row1[i],row2[i-1],weight=1,color='cyan')
        G.add_edge(row1[i],row2[i],weight=1,color='cyan')
        G.add_edge(row1[i],row2[i+1],weight=1,color='cyan')
    elif i==8:
        G.add_edge(row1[i],row2[i-1],weight=1,color='cyan')
        G.add_edge(row1[i],row2[i],weight=1,color='cyan')
    elif i==9:
        G.add_edge(row1[i],row2[i-1],weight=1,color='cyan')

# add the next row in there -- row2 with row3
for i in range(0, len(row2)):
    if i==0:
        G.add_edge(row2[i],row3[i],weight=1,color='black')
        G.add_edge(row2[i],row3[i+1],weight=1,color='black')
    elif i<= 5 :
        G.add_edge(row2[i],row3[i-1],weight=1,color='black')
        G.add_edge(row2[i],row3[i],weight=1,color='black')
        G.add_edge(row2[i],row3[i+1],weight=1,color='black')
    elif i==6:
        G.add_edge(row2[i],row3[i-1],weight=1,color='black')
        G.add_edge(row2[i],row3[i],weight=1,color='black')
    elif i==7:
        G.add_edge(row2[i],row3[i-2],weight=1,color='black')
        G.add_edge(row2[i],row3[i-1],weight=1,color='black')
    elif i==8:
        G.add_edge(row2[i],row3[i-2],weight=1,color='black')

# added all unidirected graphs with weights   

"""
#Good to see & possibly debug the structure of the graph as follows -
G.number_of_nodes()
G.number_of_edges()
list_of_all_nodes=[row1,row2,row3]
#flatten all_nodes as a single list - of which we can find the length, and see all edges together
list_of_all_nodes=list(chain.from_iterable(all_nodes))
"""

edgelist1=[(u,v) for (u,v,y) in G.edges(data=True) if y['color']=='red']
edgelist2=[(u,v) for (u,v,y) in G.edges(data=True) if y['color']=='blue']
edgelist3=[(u,v) for (u,v,y) in G.edges(data=True) if y['color']=='green']
edgelist4=[(u,v) for (u,v,y) in G.edges(data=True) if y['color']=='black']
edgelist5=[(u,v) for (u,v,y) in G.edges(data=True) if y['color']=='cyan']

pos=nx.spring_layout(G)
#color coding the rows of the keyboard differently, and picking up the labels of the nodes
nx.draw_networkx_nodes(G,pos,nodelist=row1,node_color='red',node_size=400)
nx.draw_networkx_nodes(G,pos,nodelist=row2,node_color='blue',node_size=400)
nx.draw_networkx_nodes(G,pos,nodelist=row3,node_color='green',node_size=400)

nx.draw_networkx_edges(G,pos,edgelist1,width=4,edge_color='red')
nx.draw_networkx_edges(G,pos,edgelist2,width=4,edge_color='blue')
nx.draw_networkx_edges(G,pos,edgelist3,width=4,edge_color='green')
nx.draw_networkx_edges(G,pos,edgelist4,width=3,edge_color='black')
nx.draw_networkx_edges(G,pos,edgelist5,width=3,edge_color='cyan')

# labels
nx.draw_networkx_labels(G,pos,font_size=15,font_family='calibri')

plt.axis('off')
plt.savefig("finger_keying.png")
#plt.show() # display the QWERTY represntation graph 

"""
Base assumptions for the algorithm 
1. Two words have more probabilty to be similar if the difference of their lengths does not exceed a threshold - we take this threshold as 1/3rd of the minimum of the two words keyed in
2. Will use the LevenshteinDistance if the differnce in lenghts exceeds a threshold - meaning it must not be a typo, and we must find the regular distance metric
"""

connected_node_list=G.edges()

def Qwerty_dist(word1,word2):
    dist=0
    number_passes_adjacency=0
    number_passes_direction=0
    # It can't be that most of the characters of two words are mistyped- so we prune the results, by setting another threshold for hwo many ignores are possible
    threshold=(min(len(word1),len(word2)))/3
    number_passes_direction_threshold= (min(len(word1),len(word2)))/2
    number_passes_adjacency_threshold= (min(len(word1),len(word2)))/2
    if abs(len(word1)-len(word2))<=threshold:
        for i in range(0,min(len(word1),len(word2))):
            if word1[i]==word2[i]:
                pass
            elif (word1[i],word2[i]) or (word2[i],word1[i]) in connected_node_list :
                #meaning the intention of the user must have been same and it's indeed a typo
                number_passes_adjacency=number_passes_adjacency+1
                #pass
            elif (word1[i],word2[i]) or (word2[i],word1[i]) not in connected_node_list :
            #looks like the characters in the words just got reversed, we place the restriction that the two swapped "keys" happen to come from left & right section of keyboard
                if i<=min(len(word1),len(word2))-2 :
                    if (word1[i] in left_keys and word2[i] in right_keys) or (word1[i] in right_keys and word2[i] in left_keys):
                        #either the keys are exactly reversed 
                        if word1[i]==word2[i+1] and word1[i+1]==word2[i]:
                            number_passes_direction=number_passes_direction+1
                            #pass
                        #otherwise, it could be that the "swapped" words are "neighborhood" connected, hence looking up in connected_node_list- ie we swapped, but we did a typo !
                        elif (word1[i],word2[i+1]) and (word1[i+1],word2[i]) in connected_node_list :
                            number_passes_adjacency=number_passes_adjacency+1
                            #pass
                        else :
                            dist = dist+1
            else: 
                dist=dist+1
        if dist ==0 and number_passes_direction <= number_passes_direction_threshold  and number_passes_adjacency <= number_passes_adjacency_threshold :
            return 0;
        elif dist>0 and number_passes_direction <= number_passes_direction_threshold  and number_passes_adjacency <= number_passes_adjacency_threshold :
            return dist;
        elif number_passes_direction > number_passes_direction_threshold  or number_passes_adjacency > number_passes_adjacency_threshold :
            dist=editDistDP(word1,word2)
            return dist;
    else:
    #if abs(len(word1)-len(word2))>threshold :
        dist=editDistDP(word1,word2)
        return dist;

corrected_queries_qwerty = {}
from prettytable import PrettyTable
t = PrettyTable(['Query IDs', 'Typo', 'Term', 'Distance'])
for i in spell_error_queries.keys():
  corrected_queries_qwerty[i] = []
  for word in spell_error_queries[i]:
    mini = 5
    corr_word = word
    listRow = [];
    listRow.append(i);
    listRow.append(word);
    for original_word in myInvertedIndex2.final_dict.keys():
      if original_word in STOP_WORDS:
        continue
      temp = Qwerty_dist(word, original_word)
      if (temp<mini):
        mini = temp
        corr_word = original_word
    listRow.append(corr_word);
    listRow.append(mini);
    corrected_queries_qwerty[i].append(corr_word)
    if corr_word != word:
      t.add_row(listRow);
print(t);

precision_k_spell_qwerty={}
total=0
rel=0
for qid,q in corrected_queries_qwerty.items():
  total=0
  rel=0
  results=TFIDFRetrieval(q,documents)
  precision_k_spell_qwerty[qid]=[]
  for i in range(20):
    total+=1
    if results[i][1] in rel_docs_dict:
      for s in rel_docs_dict[results[i][1]]:
        if(s==qid):
          rel+=1
          break
    precision_k_spell_qwerty[qid].append(rel/total)

prec_5_corrected_qwerty = []
for i in precision_k_spell_with_error.keys():
  prec_5_corrected_qwerty.append(precision_k_spell_qwerty[i][4])
plt.plot(queryID,prec_5_error,label = "Original Precision")
plt.plot(queryID,prec_5_corrected_qwerty,label = "Improved Precision")
plt.xlabel('QueryID') 
plt.ylabel('Performance scores') 
plt.title('Performance Graph for K=5')
plt.legend() 
plt.figure(figsize = (20, 18))
plt.show()


prec_10_corrected_qwerty = []
for i in precision_k_spell_with_error.keys():
  prec_10_corrected_qwerty.append(precision_k_spell_qwerty[i][9])
plt.plot(queryID,prec_10_error,label = "Original Precision")
plt.plot(queryID,prec_10_corrected_qwerty,label = "Improved Precision")
plt.xlabel('QueryID') 
plt.ylabel('Performance scores') 
plt.title('Performance Graph for K=5')
plt.legend() 
plt.figure(figsize = (20, 18))
plt.show()

"""**Task 3**"""



from time import process_time

def tf_idf_dict_ret(docs):
  tf_dict = {}
  for docId in docs.keys():
    tf_dict[docId] = get_tf_doc(docs,docId)

    
  df_dict = get_df(docs)
  idf_dict = {}
  for key,val in df_dict.items():
    idf_dict[key] = log10(len(spacyDocs)/float(val))
  return (tf_dict,idf_dict)

def tf_idf_dict_ret_p(docs):
  tf_dict = {}
  for docId in docs.keys():
    tf_dict[docId] = get_tf_doc_p(docs,docId)

    
  df_dict = get_df_p(docs)
  idf_dict = {}
  for key,val in df_dict.items():
    idf_dict[key] = log10(len(spacyDocs)/float(val))
  return (tf_dict,idf_dict)

def get_time(query_dict,docs):
  mfull_time={}
  mfull_tf_dict={}
  mfull_idf_dict={}
  mfull_tf_dict,mfull_idf_dict =tf_idf_dict_ret(docs)
  #print(mfull_idf_dict)
  #print(mfull_tf_dict)
  start=process_time()
  for qid,q in query_dict.items():
    results=TFIDFRetrieval(q,docs,mfull_tf_dict,mfull_idf_dict)
    end=process_time()
    mfull_time[qid]=end-start
    start=process_time()
  return mfull_time

"""Time for Mfull

"""

mfull_time = get_time(post_stop_removal2 ,documents)

mfull_time_optimized = get_time(post_stop_removal2,post_stop_removal2)

labels=[]
time=[]
time_optimized = []
#k=2
for qid in post_stop_removal2.keys():
  labels.append(qid)
  time.append(mfull_time[qid]*1000)
  time_optimized.append(mfull_time_optimized[qid]*1000)


x = np.arange(len(labels))*2  # the label locations
width = 0.6  # the width of the bars

fig, ax = plt.subplots()
rects1 = ax.bar(x - width/2, time, width, label='Original Time')
rects2 = ax.bar(x + width/2, time_optimized, width, label='Optimal Time')

# Add some text for labels, title and custom x-axis tick labels, etc.
ax.set_ylabel('Time(ms)')
ax.set_title('Time optimization for Mfull')
ax.set_xticks(x)
ax.set_xticklabels(labels)
ax.legend()


def autolabel(rects):
    """Attach a text label above each bar in *rects*, displaying its height."""
    for rect in rects:
        height = rect.get_height()
        ax.annotate('{}'.format(truncate(height,3)),
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')


autolabel(rects1)
autolabel(rects2)

fig.tight_layout()
fig.set_size_inches(16, 8)
plt.show()

"""Time for Mp

"""

mp_time = get_time(stemmed_queries2 ,documents)

mp_time_optimized = get_time(stemmed_queries2 ,stemmed_queries2)

labels=[]
time=[]
time_optimized = []
#k=2
for qid in stemmed_queries2.keys():
  labels.append(qid)
  time.append(mp_time[qid]*1000)
  time_optimized.append(mp_time_optimized[qid]*1000)


x = np.arange(len(labels))*2  # the label locations
width = 0.6  # the width of the bars

fig, ax = plt.subplots()
rects1 = ax.bar(x - width/2, time, width, label='Original Time')
rects2 = ax.bar(x + width/2, time_optimized, width, label='Optimal Time')

# Add some text for labels, title and custom x-axis tick labels, etc.
ax.set_ylabel('Time(ms)')
ax.set_title('Time optimization for Mp')
ax.set_xticks(x)
ax.set_xticklabels(labels)
ax.legend()


def autolabel(rects):
    """Attach a text label above each bar in *rects*, displaying its height."""
    for rect in rects:
        height = rect.get_height()
        ax.annotate('{}'.format(truncate(height,3)),
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')


autolabel(rects1)
autolabel(rects2)

fig.tight_layout()
fig.set_size_inches(16, 8)
plt.show()

"""Time for Mspell"""

mspell_time = get_time(corrected_queries ,documents)

mspell_time_optimized = get_time(corrected_queries ,corrected_queries)

labels=[]
time=[]
time_optimized = []
#k=2
for qid in corrected_queries.keys():
  labels.append(qid)
  time.append(mspell_time[qid]*1000)
  time_optimized.append(mspell_time_optimized[qid]*1000)


x = np.arange(len(labels))*2  # the label locations
width = 0.6  # the width of the bars

fig, ax = plt.subplots()
rects1 = ax.bar(x - width/2, time, width, label='Original Time')
rects2 = ax.bar(x + width/2, time_optimized, width, label='Optimal Time')

# Add some text for labels, title and custom x-axis tick labels, etc.
ax.set_ylabel('Time(ms)',fontsize = 20)
ax.set_title('Time optimization for Mspell',fontsize = 20)
ax.set_xticks(x)
ax.set_xticklabels(labels,fontsize = 20)
ax.legend()


def autolabel(rects):
    """Attach a text label above each bar in *rects*, displaying its height."""
    for rect in rects:
        height = rect.get_height()
        ax.annotate('{}'.format(truncate(height,3)),
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')


autolabel(rects1)
autolabel(rects2)

fig.tight_layout()
fig.set_size_inches(16, 8)
plt.show()

"""Time for Mqwerty"""

mqwerty_time = get_time(corrected_queries_qwerty ,documents)

mqwerty_time_optimized = get_time(corrected_queries_qwerty ,corrected_queries_qwerty)

labels=[]
time=[]
time_optimized = []
#k=2
for qid in corrected_queries.keys():
  labels.append(qid)
  time.append(mqwerty_time[qid]*1000)
  time_optimized.append(mqwerty_time_optimized[qid]*1000)


x = np.arange(len(labels))*2  # the label locations
width = 0.6  # the width of the bars

fig, ax = plt.subplots()
rects1 = ax.bar(x - width/2, time, width, label='Original Time')
rects2 = ax.bar(x + width/2, time_optimized, width, label='Optimal Time')

# Add some text for labels, title and custom x-axis tick labels, etc.
ax.set_ylabel('Time(ms)',fontsize = 20)
ax.set_title('Time optimization for Mqwerty',fontsize = 20)
ax.set_xticks(x)
ax.set_xticklabels(labels,fontsize = 20)
ax.legend()


def autolabel(rects):
    """Attach a text label above each bar in *rects*, displaying its height."""
    for rect in rects:
        height = rect.get_height()
        ax.annotate('{}'.format(truncate(height,3)),
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')


autolabel(rects1)
autolabel(rects2)

fig.tight_layout()
fig.set_size_inches(16, 8)
plt.show()

